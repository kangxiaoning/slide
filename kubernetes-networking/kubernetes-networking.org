#+Title: Kubernetes Networking
#+Author: kangxiaoning

#+REVEAL_THEME: white
#+REVEAL_PLUGINS: (highlight)
#+OPTIONS: reveal_slide_number:c/t toc:nil num:nil reveal_width:1400 reveal_height:1000
#+OPTIONS: reveal_single_file:t
#+REVEAL_HIGHLIGHT_CSS: https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.1/styles/github.min.css



* Objectives
  #+ATTR_REVEAL: :frag (appear)
  + Understand networking and Linux primitives that power Kubernetes
  + Understand the Container Network Interface (CNI) project
  + Understand the Kubernetes networking model


* Agenda
  + TCP/IP and OSI
  + Linux Networking
  + Kubernetes networking model


* TCP/IP and OSI
  - IP 
  - TCP
  - TLS 
  - VXLAN

** 
  :PROPERTIES:
  :reveal_background: ./images/tcpip-and-osi.png
  :reveal_background_size: 500px
  :END:

** IP Header
*** 
    :PROPERTIES:
    :reveal_background: ./images/ipv4-header.svg
    :reveal_background_size: 1000px
    :END:

*** IP Header fields

    #+REVEAL_HTML: <div style="font-size: 90%;">
    - *Internet Header Length (IHL)*
      - The IPv4 header has a variable size due to the optional 14th field option.
    - *Identification*
      - This is the identification field and is used for uniquely identifying the group of fragments of a single IP datagram.
    - *Flags*
      - This is used to control or identify fragments. In order from most significant to least:
      - bit 0: Reserved, set to zero
      - bit 1: Do not Fragment
      - bit 2: More Fragments
    - *Time To Live (TTL)*
      - An 8-bit time to live field helps prevent datagrams from going in circles on a network.
    #+REVEAL_HTML: </div>

*** IP Header in wireshark
*** 
    :PROPERTIES:
    :reveal_background: ./images/ipv4-header-wireshark.png
    :reveal_background_size: 1400px
    :END:

** TCP Header
*** 
    :PROPERTIES:
    :reveal_background: ./images/tcp-header.svg
    :reveal_background_size: 1000px
    :END:

*** TCP Header fields
    #+REVEAL_HTML: <div style="font-size: 90%;">
    - *Sequence number (32 bits)*
      - If the SYN flag is set, this is the initial sequence number. The sequence number of the first data byte and the acknowledged number in the corresponding ACK is this sequence number plus 1. It is also used to reassemble data if it arrives out of order.
    - *Acknowledgment number (32 bits)*
      - If the ACK flag is set, then this field’s value is the next sequence number of the ACK the sender is expecting. This acknowledges receipt of all preceding bytes (if any). Each end’s first ACK acknowledges the other end’s initial sequence number itself, but no data has been sent.
    - *Window size (16 bits)*
      - This is the size of the receive window.
    - *Options*
      - Variable 0–320 bits, in units of 32 bits.
    - *Padding*
      - The TCP header padding is used to ensure that the TCP header ends, and data begins on a 32-bit boundary.
    #+REVEAL_HTML: </div>

    #+REVEAL: split

    #+REVEAL_HTML: <div style="font-size: 90%;">
    - *Flags (9 bits)*
      - *NS–ECN-nonce*: Concealment protection.
      - *CWR*: Congestion Window Reduced; the sender reduced its sending rate.
      - *ECE*: ECN Echo; the sender received an earlier congestion notification.
      - *URG*: Urgent; the Urgent Pointer field is valid, but this is rarely used.
      - *ACK*: Acknowledgment; the Acknowledgment Number field is valid and is always on after a connection is established.
      - *PSH*: Push; the receiver should pass this data to the application as soon as possible.
      - *RST*: Reset the connection or connection abort, usually because of an error.
      - *SYN*: Synchronize sequence numbers to initiate a connection.
      - *FIN*: The sender of the segment is finished sending data to its peer.
    #+REVEAL_HTML: </div>

*** TCP Header in wireshark
*** 
    :PROPERTIES:
    :reveal_background: ./images/tcp-header-wireshark.png
    :reveal_background_trans: slide
    :END:

*** TCP State
*** 
    :PROPERTIES:
    :reveal_background: ./images/tcp-state.svg
    :reveal_background_size: 1000px
    :END:

*** TCP State

    #+REVEAL_HTML: <div style="font-size: 90%;">
    - *LISTEN* (server)
      - Represents waiting for a connection request from any remote TCP and port

    - *SYN-SENT* (client)
      - Represents waiting for a matching connection request after sending a connection request

    - *SYN-RECEIVED* (server)
      - Represents waiting for a confirming connection request acknowledgment after having both received and sent a connection request

    - *ESTABLISHED* (both server and client)
      - Represents an open connection; data received can be delivered to the user—the intermediate state for the data transfer phase of the connection

    - *FIN-WAIT-1* (both server and client)
      - Represents waiting for a connection termination request from the remote host

    - *FIN-WAIT-2* (both server and client)
      - Represents waiting for a connection termination request from the remote TCP
    #+REVEAL_HTML: </div>

    #+REVEAL: split

    #+REVEAL_HTML: <div style="font-size: 90%;">
    - *CLOSE-WAIT* (both server and client)
      - Represents waiting for a local user’s connection termination request

    - *CLOSING* (both server and client)
      - Represents waiting for a connection termination request acknowledgment from the remote TCP

    - *LAST-ACK* (both server and client)
      - Represents waiting for an acknowledgment of the connection termination request previously sent to the remote host

    - *TIME-WAIT* (either server or client)
      - Represents waiting for enough time to pass to ensure the remote host received the acknowledgment of its connection termination request

    - *CLOSED* (both server and client)
      - Represents no connection state at all
    #+REVEAL_HTML: </div>

** TLS Handshake
*** 
   :PROPERTIES:
   :reveal_background: ./images/tls-handshake.png
   :reveal_background_size: 800px
   :END:

*** TLS Handshake
   #+REVEAL_HTML: <div style="font-size: 90%;">
   - 1. *ClientHello*: This contains the cipher suites supported by the client and a random number.
   - 2. *ServerHello*: This message contains the cipher it supports and a random number.
   - 3. *ServerCertificate*: This contains the server’s certificate and its server public key.
   - 4. *ServerHelloDone*: This is the end of the ServerHello. If the client receives a request for its certificate, it sends a ClientCertificate message.
   - 5. *ClientKeyExchange*: Based on the server’s random number, our client generates a random premaster secret, encrypts it with the server’s public key certificate, and sends it to the server.
   - 6. *Key Generation*: The client and server generate a master secret from the premaster secret and exchange random values.
   - 7. *ChangeCipherSpec*: Now the client and server swap their ChangeCipherSpec to begin using the new keys for encryption.
   - 8. *Finished Client*: The client sends the finished message to confirm that the key exchange and authentication were successful.
   - 9. *Finished Server*: Now, the server sends the finished message to the client to end the handshake.
   #+REVEAL_HTML: </div>

*** 
   :PROPERTIES:
   :reveal_background: ./images/tls-01.png
   :END:

*** 
   :PROPERTIES:
   :reveal_background: ./images/tls-02.png
   :END:

*** 
   :PROPERTIES:
   :reveal_background: ./images/tls-03.png
   :END:

*** 
   :PROPERTIES:
   :reveal_background: ./images/tls-04.png
   :END:

** VXLAN Header
*** 
   :PROPERTIES:
   :reveal_background: ./images/vxlan-header.png
   :reveal_background_size: 1000px
   :END:

*** VXLAN Header in wireshark
*** 
   :PROPERTIES:
   :reveal_background: ./images/vxlan-header-wireshark.png
   :reveal_background_trans: slide
   :END:


* Linux Networking
  - Linux network devices
  - Netfilter
  - Ipvs
  - eBPF

** Linux network devices
*** bridge and veth
    #+REVEAL_HTML: <div style="font-size: 90%;">
    - *Bridge*
      - the bridge functions like a network switch between network interfaces on a host, seamlessly connecting them.

    #+ATTR_REVEAL: :code_attribs data-line-numbers
    #+begin_src bash
      # Add a new bridge interface named br0.
      ip link add br0 type bridge
      # Attach eth0 to our bridge.
      ip link set eth0 master br0
      # Attach veth to our bridge.
      ip link set veth master br0
    #+end_src

    - *Veth*
      - The veth device is a local Ethernet tunnel. Veth devices are created in pairs. Packets transmitted on one device in the pair are immediately received on the other device. When either device is down, the link state of the pair is down. Adding a bridge to Linux can be done with using the ~brctl~ commands or ~ip~.

    #+ATTR_REVEAL: :code_attribs data-line-numbers
    #+begin_src bash
      ip netns add net1
      ip netns add net2
      ip link add veth1 netns net1 type veth peer name veth2 netns net2
    #+end_src
    #+REVEAL_HTML: </div>
*** 
    :PROPERTIES:
    :reveal_background: ./images/linux-bridge-interface.png
    :reveal_background_size: 500px
    :END:

*** tun and tap

    #+REVEAL_HTML: <div style="font-size: 90%;">
    - *TUN*
      - tunnel devices operate at *layer 3*, meaning the data (packets) you will receive from the file descriptor will be IP based. Data written back to the device must also be in the form of an IP packet.
    - *TAP*
      - network tap operates much like TUN however instead of only being able to write and receive layer 3 packets to/from the file descriptor it can use *raw ethernet* packets. You will typically see TAP devices used by KVM/Qemu virtualization, where a TAP device is assigned to a virtual guest interface during creation.
    #+REVEAL_HTML: </div>

*** 
   :PROPERTIES:
   :reveal_background: ./images/linux-network-device.png
   :reveal_background_size: 1200px
   :END:

** Netfilter

   #+REVEAL_HTML: <div style="font-size: 90%;">
   - The *netfilter project* is a community-driven collaborative FOSS project that provides packet filtering software for the Linux 2.4.x and later kernel series. The netfilter project is commonly associated with iptables and its successor nftables.
   - The *netfilter* project enables packet filtering, network address [and port] translation (NA[P]T), packet logging, userspace packet queueing and other packet mangling.
   - The *netfilter hooks* are a framework inside the Linux kernel that allows kernel modules to register callback functions at different locations of the Linux network stack. The registered callback function is then called back for every packet that traverses the respective hook within the Linux network stack.
   - *iptables* is a generic firewalling software that allows you to define rulesets. Each rule within an IP table consists of a number of classifiers (iptables matches) and one connected action (iptables target).
   #+REVEAL_HTML: </div>

   #+REVEAL: split

   #+REVEAL_HTML: <div style="font-size: 90%; text-align: left;">
   - *nftables* is the successor of iptables, it allows for much more flexible, scalable and performance packet classification. This is where all the fancy new features are developed.

   reference:
     - https://www.teldat.com/blog/nftables-and-netfilter-hooks-via-linux-kernel/
     - https://www.netfilter.org/
     - https://www.fatalerrors.org/a/0tx80Dw.html
   #+REVEAL_HTML: </div>

*** 
   :PROPERTIES:
   :reveal_background: ./images/netfilter-framework.jpeg
   :reveal_background_size: 1000px
   :END:

*** 
   :PROPERTIES:
   :reveal_background: ./images/packet-flow-in-netfilter.svg
   :reveal_background_size: 1400px
   :END:

*** Netfilter hooks
    #+REVEAL_HTML: <div style="font-size: 70%;">
    | Netfilter hook       | Iptables chain name | Description                                                                                   |
    |----------------------+---------------------+-----------------------------------------------------------------------------------------------|
    | <20>                 | <13>                |                                                                                               |
    | ~NF_IP_PRE_ROUTING~  | PREROUTING          | Triggers when a packet arrives from an external system.                                       |
    | ~NF_IP_LOCAL_IN~     | INPUT               | Triggers when a packet’s destination IP address matches this machine.                         |
    | ~NF_IP_FORWARD~      | NAT                 | Triggers for packets where neither source nor destination matches the machine’s IP addresses. |
    | ~NF_IP_LOCAL_OUT~    | OUTPUT              | Triggers when a packet, originating from the machine, is leaving the machine.                 |
    | ~NF_IP_POST_ROUTING~ | POSTROUTING         | Triggers when any packet (regardless of origin) is leaving the machine.                       |
    #+REVEAL_HTML: </div>

*** 
   :PROPERTIES:
   :reveal_background: ./images/iptables-table-chain.png
   :reveal_background_size: 1000px
   :END:

*** data structure

    #+REVEAL_HTML: <div style="font-size: 90%; text-align: left;">
    The structure corresponds to the tables in iptables. At present, the kernel registers tables of filter, mangle, nat and raw. 

    #+ATTR_REVEAL: :code_attribs data-line-numbers="1,15-16"
    #+begin_src c
      struct xt_table
      {
        //All the tables are registered in the xt linked list of the network namespace;
        struct list_head list;
      
        //Name of table, such as "filter", "nat","mangle"
        const char name[XT_TABLE_MAXNAMELEN];
      
        //It is used by bit to indicate which HOOK point rules and 5 hooks are saved in the table
        unsigned int valid_hooks;
      
        //This lock protects the contents of the private pointer in the table
        rwlock_t lock;
      
        //It actually points to struct xt_table_info object, as defined below
        void *private;
      
        /* Set this to THIS_MODULE if you are a module, otherwise NULL */
        struct module *me;
      
        //Which protocol family does the table belong to
        int af;		/* address/protocol family */
      };
      
      struct net {
        ...
      #ifdef CONFIG_NETFILTER
        struct netns_xt		xt;
      #endif
      };
      struct netns_xt {
        struct list_head tables[NPROTO];
      };
    #+end_src

    source code: https://elixir.bootlin.com/linux/latest/source/include/linux/netfilter/x_tables.h#L223
    #+REVEAL_HTML: </div>

    #+REVEAL: split

    #+REVEAL_HTML: <div style="font-size: 90%; text-align: left;">
    Structure ~xt_table~ is only a basic description of table and does not save the rules in table. More table contents are in its private member, and the structure pointed to by this member is struct ~xt_talble_info~ object, which is defined as follows.

    #+ATTR_REVEAL: :code_attribs data-line-numbers="1,17-20"
    #+begin_src c
      struct xt_table_info
      {
        //Memory size of all rules in table
        unsigned int size;
        //Number of rules currently saved in table
        unsigned int number;
        //The number of rules in the table at initial registration
        /* Initial number of entries. Needed for module usage count */
        unsigned int initial_entries;
        //Each table can save rules of multiple HOOK points (by the table's valid_hook),
        //After entering from a HOOK point, only the rules on the HOOK point should be checked when checking rules
        //In order to delimit the rules in the table according to the HOOK point, there are two members as follows:
        //hook_entry [] records the offset of the first rule of each HOOK point from entries;
        //Under flow [] records the offset of the last rule of each HOOK point from the entries
        unsigned int hook_entry[NF_INET_NUMHOOKS];
        unsigned int underflow[NF_INET_NUMHOOKS];
        //The rule in table is that each CPU has a copy. This is to avoid mutual exclusion between multiple CPUs,
        //Therefore, the entries are allocated according to the number of CPUs. For example, if there are two CPUs, two char pointers will be allocated
        //The CPU ID is used to index
        char *entries[1];
      };
      //The following auxiliary macro is used to calculate struct XT_ table_ The memory size of the info object,
      //Note that the entries have been sized according to the number of CPU s
      #define XT_TABLE_INFO_SZ (offsetof(struct xt_table_info, entries) \
                                + nr_cpu_ids * sizeof(char *))
    #+end_src
    #+REVEAL_HTML: </div>

*** netfilter table chain
*** 
    :PROPERTIES:
    :reveal_background: ./images/netfilter-table-chain.jpeg
    :reveal_background_size: 1200px
    :END:

** Ipvs
   - *DR* directly routes packets to the backend server by rewriting the MAC address of the data frame with the MAC address of the selected backend server.
   - *IP tunneling* encapsulates IP datagrams within IP datagrams.
   - *NAT* rewrites source and destination addresses.

   - reference:
     - http://www.linuxvirtualserver.org/Documents.html
     - https://debugged.it/blog/ipvs-the-linux-load-balancer/

*** 
    :PROPERTIES:
    :reveal_background: ./images/ipvs-dr-mode.svg
    :reveal_background_size: 1000px
    :END:

*** 
    :PROPERTIES:
    :reveal_background: ./images/ipvs-nat-mode.svg
    :reveal_background_size: 600px
    :END:

*** 
    :PROPERTIES:
    :reveal_background: ./images/ipvs-tun-mode.svg
    :reveal_background_size: 1200px
    :END:

*** iptables issues

    #+REVEAL_HTML: <div style="font-size: 90%; text-align: left;">
    There are three aspects to look at when it comes to issues with iptables as a load balancer:

    - Number of nodes in the cluster
      - Even though Kubernetes already supports 5,000 nodes in release v1.6, kube-proxy with iptables is a bottleneck to scale the cluster to 5,000 nodes. One example is that with a NodePort service in a 5,000-node cluster, if we have 2,000 services and each service has 10 pods, this will cause at least 20,000 iptables records on each worker node, which can make the kernel pretty busy.

    - Time
      - The time spent to add one rule when there are 5,000 services (40,000 rules) is 11 minutes. For 20,000 services (160,000 rules), it’s 5 hours.

    - Latency
      - There is latency to access a service (routing latency); each packet must traverse the iptables list until a match is made. There is latency to add/remove rules, inserting and removing from an extensive list is an intensive operation at scale.

    #+REVEAL_HTML: </div>
*** 
    :PROPERTIES:
    :reveal_background: ./images/kubernetes-ipvs.png
    :reveal_background_size: 800px
    :END:

** eBPF

   #+REVEAL_HTML: <div style="font-size: 90%; text-align: left;">
   *eBPF* is a programming system that allows special sandboxed programs to run in the kernel without passing back and forth between kernel and user space, like we saw with Netfilter and iptables.

   In addition to socket filtering, other supported attach points in the kernel are as follows:

   - *Kprobes*
     - Dynamic kernel tracing of internal kernel components.

   - *Uprobes*
     - User-space tracing.

   - *Tracepoints*
     - Kernel static tracing. These are programed into the kernel by developers and are more stable as compared to kprobes, which may change between kernel versions.

   - *perf_events*
     - Timed sampling of data and events.

   - *XDP*
     - Specialized eBPF programs that can go lower than kernel space to access driver space to act directly on packets.

   #+REVEAL_HTML: </div>

*** 
    :PROPERTIES:
    :reveal_background: ./images/ebpf-example.png
    :reveal_background_size: 600px
    :END:

*** 
    :PROPERTIES:
    :reveal_background: ./images/cilium-interacts-with-ebpf.png
    :reveal_background_size: 1000px
    :END:


* Kubernetes networking model
  
  - Pod Networking
  - Service Routing

** Pod Networking
   - CNI(Container Networking Interface)
   - Overlay Networking
   - Routing

*** Concepts
    #+REVEAL_HTML: <div style="font-size: 90%; text-align: left;">
    - *VTEP* 
      - VXLAN Tunnel Endpoints, performs data frame encapsulation and decapsulation. The VTEP peer interaction ensures that the data gets forwarded to the relevant destination container addresses. The data leaving the containers is encapsulated with VXLAN information and transferred over the VXLAN tunnels to be de-encapsulated by the peer VTEP.
    #+REVEAL_HTML: </div>

*** CNI
    #+REVEAL_HTML: <div style="font-size: 90%; text-align: left;">
    Kubernetes relies on the CNI project to comply with the following requirements:

    - All containers must communicate with each other without NAT.
    - Nodes can communicate with containers without NAT.
    - A container’s IP address is the same as those outside the container that it sees itself as.
    #+REVEAL_HTML: </div>

    #+REVEAL: split

    #+REVEAL_HTML: <div style="font-size: 90%; text-align: left;">
    - *CNI*
      - The CNI specification defines a few key operations:

      - ADD
        - Adds a container to the network and responds with the associated interface(s), IP(s), and more.

      - DELETE
        - Removes a container from the network and releases all associated resources.

      - CHECK
        - Verifies a container’s network is set up properly and responds with an error if there are issues.

      - VERSION
        - Returns the CNI version(s) supported by the plug-in.
    #+REVEAL_HTML: </div>

*** Overlay Networking
*** 
   :PROPERTIES:
   :reveal_background: ./images/vxlan-tunnel-detailed.png
   :reveal_background_size: 800px
   :END:
*** 
   :PROPERTIES:
   :reveal_background: ./images/vxlan-encapsulation.png
   :reveal_background_size: 800px
   :END:

*** Routing

    #+REVEAL_HTML: <div style="font-size: 90%; text-align: left;">
    *Border Gateway Protocol (BGP)* is one of the most commonly used protocols to distribute workload routes. It is used in projects such as Calico and Kube-Router. Not only does BGP enable communication of workload routes in the cluster but its internal routers can also be peered with external routers. Doing so can make external network fabrics aware of how to route to Pod IPs.

    #+REVEAL_HTML: </div>
*** 
   :PROPERTIES:
   :reveal_background: ./images/calico-routing.png
   :reveal_background_size: 800px
   :END:

*** Calico

    #+REVEAL_HTML: <div style="font-size: 90%; text-align: left;">
    Calico offers a variety of ways to route packets inside of the cluster. This includes:

    - Native
      - No encapsulation of packets.

    - IP-in-IP
      - Simple encapsulation. IP packet is placed in the payload of another.

    - VXLAN
      - Advanced encapsulation. An entire L2 frame is encapsulated within a UDP packet. Establishes a virtual L2 overlay.the VXLAN mode does not require usage of BGP, which can be a solution to environments where BGP peering is blocked.

    #+REVEAL_HTML: </div>
*** 
   :PROPERTIES:
   :reveal_background: ./images/calico-component-relationship.png
   :reveal_background_size: 800px
   :END:

*** 
   :PROPERTIES:
   :reveal_background: ./images/calico-crosssubnet-mode.png
   :reveal_background_size: 800px
   :END:

** Service Routing

   - ClusterIP
   - NodePort
   - LoadBalancer
   - Ingress

*** Service Implementation
    - Kube-proxy
      - Kube-proxy is an agent that runs on every cluster node. It is primarily responsible for making Services available to the Pods running on the local node. It achieves this by watching the API server for Services and Endpoints and programming the Linux networking stack (using iptables, for example) to handle packets accordingly.
      - Historically, kube-proxy acted as a network proxy between Pods running on the node and Services. This is where the kube-proxy name came from. As the Kubernetes project evolved, however, kube-proxy stopped being a proxy and became more of a node agent or localized control plane.

*** ClusterIP
    - ClusterIP Service creates a virtual IP address (VIP) that is backed by one or more Pods. Usually, the VIP is available only to workloads running inside the cluster.

*** 
   :PROPERTIES:
   :reveal_background: ./images/clusterip-service.png
   :reveal_background_size: 800px
   :END:

*** NodePort

    #+REVEAL_HTML: <div style="font-size: 90%; text-align: left;">
    The *NodePort* Service is useful when you need to expose a Service to network clients outside of the cluster, such as existing applications running in VMs or users of a web application.

    As the name suggests, the NodePort Service exposes the Service on a port across all cluster nodes. The port is assigned randomly from a configurable port range. Once assigned, all nodes in the cluster listen for connections on the given port.
    #+REVEAL_HTML: </div>
*** 
   :PROPERTIES:
   :reveal_background: ./images/nodeport-service.png
   :reveal_background_size: 800px
   :END:

*** LoadBalancer

    #+REVEAL_HTML: <div style="font-size: 90%; text-align: left;">
    The *LoadBalancer* Service builds upon the NodePort Service to address some of its downsides. At its core, the LoadBalancer Service is a NodePort Service under the hood. However, the LoadBalancer Service has additional functionality that is satisfied by a controller.

    The controller, also known as a cloud provider integration, is responsible for automatically gluing the NodePort Service with an external load balancer. In other words, the controller takes care of creating, managing, and configuring external load balancers in response to the configuration of LoadBalancer Services in the cluster. The controller does this by interacting with APIs that provision or configure load balancers.
    #+REVEAL_HTML: </div>

*** 
   :PROPERTIES:
   :reveal_background: ./images/loadbalancer-service.png
   :reveal_background_size: 800px
   :END:

*** Ingress

    #+REVEAL_HTML: <div style="font-size: 90%; text-align: left;">
    *Ingress* is a Kubernetes-specific L7 (HTTP) load balancer, which is accessible externally, contrasting with L4 ClusterIP service, which is internal to the cluster. This is the typical choice for exposing an HTTP(S) workload to external users. An ingress can be a single entry point into an API or a microservice-based architecture. Traffic can be routed to services based on HTTP information in the request. Ingress is a configuration spec (with multiple implementations) for routing HTTP traffic to Kubernetes services.
    #+REVEAL_HTML: </div>

*** 
   :PROPERTIES:
   :reveal_background: ./images/ingress-architecture.png
   :reveal_background_size: 800px
   :END:

*** 
   :PROPERTIES:
   :reveal_background: ./images/ingress-traffic.png
   :reveal_background_size: 800px
   :END:


* Q&A

